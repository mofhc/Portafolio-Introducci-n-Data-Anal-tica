# -*- coding: utf-8 -*-
"""Tarea2_WishPlatform.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K15tI9POJBMFC2o46qKxVZMRkmNuPkUK
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Necesitarás montar tu disco usando los siguientes comandos:
# Para obtener más información sobre el montaje, consulta esto: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory

from google.colab import drive
drive.mount("/content/drive")

"""# FASE 1: VISUALIZACIÓN DE DATOS"""

## La función pd.read_csv() de pandas se utiliza para leer archivos CSV (Comma-Separated Values)
## y cargar su contenido en un DataFrame, que es una estructura de datos bidimensional similar a una tabla.
data_as = pd.read_csv("/content/drive/MyDrive/Master Usac/Análisis de datos/computed_insight_success_of_active_sellers.csv", encoding='ISO-8859-1')
data_sp = pd.read_csv("/content/drive/MyDrive/Master Usac/Análisis de datos/summer-products-with-rating-and-performance_2020-08.csv", encoding='ISO-8859-1')
data_uc = pd.read_csv("/content/drive/MyDrive/Master Usac/Análisis de datos/unique-categories.csv", encoding='ISO-8859-1')
data_ucsort = pd.read_csv("/content/drive/MyDrive/Master Usac/Análisis de datos/unique-categories.sorted-by-count.csv", encoding='ISO-8859-1')

# observando los nombres de columnas de cada frame
print(data_as.columns)
print(data_sp.columns) #set de datos a utilizar
print(data_uc.columns)
print(data_ucsort.columns)

#data_as.head()
data_sp.head(5)
#data_uc.head()
#data_ucsort.head()

# Revisar tipos de datos y datos faltantes
#data_as.info()
data_sp.info()
#data_uc.head()
#data_ucsort.head()

"""# FASE 2: LIMPIEZA DE DATOS"""

#seleccionando columnas importantes
data_sp2 = data_sp.iloc[:, list(range(0,19)) + list(range(22,37)) + list(range(40,42))]

# Revisar por cualquier valor nulo
data_sp2.isnull().sum()

# Sustituyendo datos faltantes
# Si encuentra valores nulos ingresar 0, en este caso.
data_sp2['rating_five_count'] = data_sp2['rating_five_count'].fillna(0)
data_sp2['rating_four_count'] = data_sp2['rating_four_count'].fillna(0)
data_sp2['rating_three_count'] = data_sp2['rating_three_count'].fillna(0)
data_sp2['rating_two_count'] = data_sp2['rating_two_count'].fillna(0)
data_sp2['rating_one_count'] = data_sp2['rating_one_count'].fillna(0)
data_sp2['has_urgency_banner'] = data_sp2['has_urgency_banner'].fillna(0)
data_sp2['urgency_text'] = data_sp2['urgency_text'].fillna("Sin datos")
data_sp2['origin_country'] = data_sp2['origin_country'].fillna("Sin datos")
data_sp2['merchant_name'] = data_sp2['merchant_name'].fillna("Sin datos")
data_sp2['merchant_info_subtitle'] = data_sp2['merchant_info_subtitle'].fillna("Sin datos")

# Revisar por cualquier valor nulo
data_sp2.isnull().sum()

# Verifique la información del DataFrame nuevamente para confirmar que no faltan valores.
data_info = data_sp2.info()
print("\nData Info After Filling Missing Values:\n", data_info)

data_sp2['origin_country'].unique()
data_sp2['title'].unique()

#Seleccionando nuevamente columnas importantes para el análisis
col_to_use = ['title', 'title_orig', 'price', 'retail_price', 'currency_buyer', 'units_sold', 'rating', 'rating_count', 'rating_five_count', 'rating_four_count', 'rating_three_count', 'rating_two_count', 'rating_one_count', 'tags', 'shipping_option_name', 'shipping_option_price', 'shipping_is_express', 'countries_shipped_to', 'inventory_total', 'has_urgency_banner', 'urgency_text', 'origin_country', 'merchant_title', 'merchant_name', 'merchant_info_subtitle', 'merchant_rating_count', 'merchant_rating',  'merchant_id', 'product_id', 'theme']
data_sp2 = data_sp2[col_to_use]
data_sp2

"""# Fase 3 :  Previsualizar datos luego de limpieza
**Un paso fundamental en la preparación de datos**

Después de la limpieza de datos, obtener una vista previa del conjunto de datos, es un paso esencial para garantizar que el proceso de limpieza haya sido eficaz. Este paso nos permite validar que los valores faltantes, los valores atípicos y los errores se hayan abordado correctamente. También ayuda a verificar la coherencia y la estructura del conjunto de datos, y puede revelar patrones o problemas adicionales que requieran mayor atención. La vista previa de los datos garantiza que estén listos para tareas posteriores de análisis, visualización o aprendizaje automático, lo que garantiza una transición fluida a las etapas avanzadas de exploración de datos.


"""

#Tamaño del dataset de análisis
data_sp2.shape

#primeros 5 datos
data_sp2.head(5)

"""# Fase 4 : Análisis exploratorio de datos (EDA)
Ahora que hemos manejado los valores faltantes y las inconsistencias estandarizadas en el conjunto de datos, el siguiente paso es el **Análisis exploratorio de datos (EDA)**. Este paso nos ayudará a comprender mejor el conjunto de datos al visualizar y resumir los datos.

**EDA generalmente implica:**
1. **Estadísticas de resumen**: Comprender las distribuciones y las estadísticas clave.
2. **Visualización de datos**: Visualizar relaciones, distribuciones y valores atípicos.
3. **Análisis de correlación**: Verificar relaciones entre variables.

Comenzaremos con algunas **estadísticas de resumen** y **visualizaciones** básicas para descubrir patrones y posibles problemas en los datos.

## Estadísticas de resumen

Usaremos estadísticas de resumen para obtener información sobre la **edad**, las **horas de estudio**, los puntajes de **Python** y los puntajes de **DB**.
"""

#descripción general de sus columnas numéricas
data_sp2.describe().T

"""### **Análisis de las estadísticas más importantes**
- 1573 productos existen en el conjunto de datos.

### **Relación entre precio de venta y precios en rebajas**

A simple vista se puede observar que existen descuentos significativos.

- Pues, después de aplicar el descuento, el precio promedio de los productos se sitúa en 8.33 unidades, lo que es notablemente inferior al precio promedio de venta al público pues es menor a la mitad del precio de venta promedio igual a 23.28 unidades.

- La variación entre el precio de venta y el precio con descuento refleja un descuento promedio de aproximadamente el 64%

- El rango de precios de productos en rebaja puede fluctuar desde 1 unidad monetaria hasta 49 unidades, mientras que los precios de venta al público van desde 1 hasta 252.

- Si analizamos los percentiles podemos analizar que el 50% de productos se encuentra en un rango de precios rebajados que varía entre 5.81 y 11.00, mientras que su rango de precios de venta normal oscila entre 7.00 a 26.00.

### **Cantidad de productos vendidos**
- Se presenta una alta variabilidad en la cantidad de productos vendidos, pues su desviación estándar es igual 9,356.54. Esto sugiere que algunos productos se venden en grandes cantidades, mientras que otros tienen ventas mucho menores.
- El valor máximo de productos vendidos es igual a 100,000 unidades. Muy alta en comparación a la media global igual a 4339.01 productos. Tomando en cuenta esto podrían estar incluidos algunos productos muy demandantes con pocos demandantes. Se sugiere analizar más a profundidad para detectar estos productos.
-La mayoría de los productos presentan ventas moderadas, ya que la mediana de ventas (percentil 50) es de 1000 unidades, lo que indica que la mitad de los productos se venden en cantidades iguales o inferiores a este número. Además, el percentil 75 es de 5000 unidades, lo que significa que el 75% de los productos tienen ventas que no superan esta cifra, lo que sugiere que, aunque hay un pequeño porcentaje de productos con ventas muy altas, la tendencia general es de un volumen de ventas moderado.

## Visualización de distribuciones

Visualizaremos la distribución de algunas columnas clave mediante histogramas y diagramas de caja, que nos ayudarán a detectar valores atípicos o patrones interesantes.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Reemplazar valores infinitos
# Si se encuentran valores infinitos, reemplácelos con NaN,
# y luego manejarlos como manejaría los valores faltantes.
data_sp2.replace([np.inf, -np.inf], np.nan, inplace=True)
data_sp2.dropna(inplace=True)

# Establecer estilo de trama
sns.set(style="whitegrid")

# Histogramas de edad, horas de estudio, puntuaciones de Python y DB
fig, ax = plt.subplots(2, 2, figsize=(12, 8))

sns.histplot(data=data_sp2['price'], kde=True, ax=ax[0, 0]).set(title='Precios en descuento')
sns.histplot(data=data_sp2['retail_price'], kde=True, ax=ax[0, 1]).set(title='Precio de venta')
sns.histplot(data=data_sp2['units_sold'], kde=True, ax=ax[1, 0]).set(title='Volumen de venta')
sns.histplot(data=data_sp2['inventory_total'], kde=True, ax=ax[1, 1]).set(title='Inventario de productos')

plt.tight_layout()
plt.show()

"""### Interpretación de las distribuciones

- Distribución de precios y descuentos: La mayoría de los descuentos se concentran en un rango bajo, y tanto los precios de descuento como los de venta presentan una ligera asimetría positiva. Esto indica que hay más productos con descuentos bajos y precios de venta más accesibles.
- Volumen de ventas: La distribución del volumen de ventas es altamente sesgada hacia la derecha, lo que sugiere que la mayoría de los productos tienen un bajo volumen de ventas, pero existe una pequeña proporción de productos muy populares.
- Inventario: La distribución del inventario también muestra una asimetría positiva, indicando que la mayoría de los productos tienen niveles de inventario bajos.

Es probable que exista una relación entre los precios de descuento y el volumen de ventas. Sería interesante analizar si los descuentos más altos generan mayores ventas.
"""

# Matriz de correlación
corr_matrix = data_sp2[['price', 'retail_price', 'units_sold']].corr()

# Trazar el mapa de calor de la matriz de correlación
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.2)
plt.title("Correlation Matrix")
plt.show()

# Tabla de matriz de correlación:
corr_matrix

print('No existe correlación en las variables analizadas.')

top_product = data_sp2.groupby(['product_id'])['units_sold'].sum().sort_values(ascending=False)
top_product.head(10)

top_product = data_sp2.groupby(['origin_country'])['units_sold'].sum().sort_values(ascending=False)
top_product.head(10)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))  # Ajusta el tamaño de la figura para mejor legibilidad
plt.bar(top_product.index, top_product.values)
plt.title('Top ciudades de origen en que más se vende productos')
plt.xlabel('Country')
plt.ylabel('Unidades Vendidas')
plt.xticks(rotation=90, ha='right')  # Rotar las etiquetas del eje x 90 grados
plt.tight_layout()  # Ajusta el diseño para evitar superposiciones
plt.show()